# net-desc_58_35_1.txt 
#
# Format of commands are
# command arguments
# Commands:
# 1)  NetType <locallyConnected | fullyConnected>
# 2) lossFunction <sumSquare | SumSquareClass | crossEntropy>
# 3) AddLayer <Layer_Name> <Layer_Type> <Unit_Transfer_Type> <Number_units>
# 4) connectLayers <LowerLayer_name> {Units} <UpperLayerName> {Units} <ConstrainedCmdNumber>
# 5) constrainLayer BaseCmdNumber ConstrainedCmdNumber
# 6) ShareWeights LowerLayerName {units} UpperLayerName {units}
#
# Notes:
# - Not a very smart parser so every command must be on a single line
# - Commands must appear (more or less) in the order listed above
# - Commands 4, 5 and 6 only apply to locallyConnected nets
# - LayerTypes: (input, hidden, output)
# - TransferFunctionTypes (sigmoid tanh linear softmax)
# - {units} can be specified as a , separted list or a range (eg 3-15)
# - ConstrainedCmdNumber is a number that can be optionally specified for a connectLayer command. It is used
#    in the constrainLayer command
#
# ConstrainLayer and ShareWeights are 2 ways of doing the same thing, sharing weights in a
# network. They are implemented differently but should have the same functionality and some
# network architectures may only be implementable using one of the two methods
# The major implementation difference is as follows:
# 
# ConstrainLayer actually physically allocates
# all the shared weights and does the correct accounting during weight update. Used when you
# want the net to have exactly the right number of weights when each weight is shared.
#
# ShareWeights only keeps one copy of shared weights, ie count of weights is the count of unique 
# weights. It then reuses weight pointers in the shared links and does nothing special during
# weight updates. This is convenient for convolution (or space displament type architectures)
 

# 
NetType locallyConnected
# lossFunction sumSquare
#lossFunction SumSquareClass
#lossFunction crossEntropy

#
# Specify  net architecture - Convolution net
# hidden units have field of view 5. Output units have FOV 3
# Input layer 46 units per time slice x 5
#
AddLayer Input_Layer Input Linear 230

# Hidden units 23 units per slice x 3
#
AddLayer Hidden_Layer Hidden sigmoid 69 bias
#
addlayer Output_Space Output Sigmoid 1  bias


lossFunction Output_Space crossEntropy

#
# Connect up as a convolution net
#

connectLayers Input_Layer {0-137 } Hidden_Layer {0-22}
connectLayers Input_Layer {46-183 } Hidden_Layer {23-45}
connectLayers Input_Layer {92-229 } Hidden_Layer {46-68}

connectLayers Hidden_Layer {0-68 } Output_Space {0}

#
# Shared weights
#


#ShareWeights Input_Layer {0-57} Hidden_Layer { 0-34 }  Input_Layer {58-115} Hidden_Layer { 35-69 }
#ShareWeights Input_Layer {0-57} Hidden_Layer { 0-34 }  Input_Layer {116-173} Hidden_Layer { 70-104 }

#ShareWeights Input_Layer {46-115} Hidden_Layer { 0-34 }  Input_Layer {116-173} Hidden_Layer { 35-69 }
#ShareWeights Input_Layer {58-115} Hidden_Layer { 0-34 }  Input_Layer {174-231} Hidden_Layer { 70-104 }

#ShareWeights Input_Layer {116-173} Hidden_Layer { 0-34 }  Input_Layer {174-231} Hidden_Layer { 35-69 }
#ShareWeights Input_Layer {116-173} Hidden_Layer { 0-34 }  Input_Layer {232-289} Hidden_Layer { 70-104 }


ShareWeights Input_Layer {0-45} Hidden_Layer { 0-22 }  Input_Layer {46-91} Hidden_Layer { 23-45 }
ShareWeights Input_Layer {0-45} Hidden_Layer { 0-22 }  Input_Layer {92-137} Hidden_Layer { 46-68 }

ShareWeights Input_Layer {46-91} Hidden_Layer { 0-22 }  Input_Layer {92-137} Hidden_Layer { 23-45 }
ShareWeights Input_Layer {46-91} Hidden_Layer { 0-22 }  Input_Layer {138-183} Hidden_Layer { 46-68 }

ShareWeights Input_Layer {92-137} Hidden_Layer { 0-22 }  Input_Layer {138-183} Hidden_Layer { 23-45 }
ShareWeights Input_Layer {92-137} Hidden_Layer { 0-22 }  Input_Layer {184-229} Hidden_Layer { 46-68 }

