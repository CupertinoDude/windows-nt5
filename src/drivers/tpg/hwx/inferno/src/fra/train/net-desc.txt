# net-desc_37_100_1_4_195.txt
#
# Format of commands are
# command arguments
# Commands:
# 1)  NetType <locallyConnected | fullyConnected>
# 2) lossFunction <sumSquare | SumSquareClass | crossEntropy>
# 3) AddLayer <Layer_Name> <Layer_Type> <Unit_Transfer_Type> <Number_units>
# 4) connectLayers <LowerLayer_name> {Units} <UpperLayerName> {Units} <ConstrainedCmdNumber>
# 5) constrainLayer BaseCmdNumber ConstrainedCmdNumber
# 6) ShareWeights LowerLayerName {units} UpperLayerName {units}
#
# Notes:
# - Not a very smart parser so every command must be on a single line
# - Commands must appear (more or less) in the order listed above
# - Commands 4, 5 and 6 only apply to locallyConnected nets
# - LayerTypes: (input, hidden, output)
# - TransferFunctionTypes (sigmoid tanh linear softmax)
# - {units} can be specified as a , separted list or a range (eg 3-15)
# - ConstrainedCmdNumber is a number that can be optionally specified for a connectLayer command. It is used
#    in the constrainLayer command
#
# ConstrainLayer and ShareWeights are 2 ways of doing the same thing, sharing weights in a
# network. They are implemented differently but should have the same functionality and some
# network architectures may only be implementable using one of the two methods
# The major implementation difference is as follows:
# 
# ConstrainLayer actually physically allocates
# all the shared weights and does the correct accounting during weight update. Used when you
# want the net to have exactly the right number of weights when each weight is shared.
#
# ShareWeights only keeps one copy of shared weights, ie count of weights is the count of unique 
# weights. It then reuses weight pointers in the shared links and does nothing special during
# weight updates. This is convenient for convolution (or space displament type architectures)
 

# 
NetType locallyConnected
# lossFunction sumSquare
#lossFunction SumSquareClass
#lossFunction crossEntropy

#
# Specify  net architecture - Convolution net
# hidden units have field of view 5. Output units have FOV 3
# Input layer 37 units per time slice x 5
#
#AddLayer Input_Layer Input Linear 150
AddLayer Input_Layer Input Linear 185

# Hidden units 100 units per slice x 3
#
AddLayer Hidden_Layer Hidden sigmoid 300 bias
#
addlayer Output_Space Output Sigmoid 1  bias
addLayer Accent_Layer Output Sigmoid 4 bias
addlayer Output_Layer Output Linear 195  bias


lossFunction Output_Space crossEntropy
lossFunction Accent_Layer sumSquare
lossFunction Output_Layer crossEntropyFull

#
# Connect up as a convolution net
#

connectLayers Input_Layer {0-110 } Hidden_Layer {0-99}
connectLayers Input_Layer {37-147 } Hidden_Layer {100-199}
connectLayers Input_Layer {74-184 } Hidden_Layer {200-299}

connectLayers Hidden_Layer {0-299 } Output_Space {0}
connectLayers Hidden_Layer {0-299 } Accent_Layer {0-3}
connectLayers Hidden_Layer {0-299 } Output_Layer {0-194}

#
# Shared weights
#

#ShareWeights Input_Layer {0-36} Hidden_Layer { 0-149 }  Input_Layer {37-73} Hidden_Layer { 150-299 }
#ShareWeights Input_Layer {0-36} Hidden_Layer { 0-149 }  Input_Layer {74-110} Hidden_Layer { 300-449 }

#ShareWeights Input_Layer {37-73} Hidden_Layer { 0-149 }  Input_Layer {74-110} Hidden_Layer { 150-299 }
#ShareWeights Input_Layer {37-73} Hidden_Layer { 0-149 }  Input_Layer {111-147} Hidden_Layer { 300-449 }

#ShareWeights Input_Layer {74-110} Hidden_Layer { 0-149 }  Input_Layer {111-147} Hidden_Layer { 150-299 }
#ShareWeights Input_Layer {74-110} Hidden_Layer { 0-149 }  Input_Layer {148-184} Hidden_Layer { 300-449 }


ShareWeights Input_Layer {0-36} Hidden_Layer { 0-99 }  Input_Layer {37-73} Hidden_Layer { 100-199 }
ShareWeights Input_Layer {0-36} Hidden_Layer { 0-99 }  Input_Layer {74-110} Hidden_Layer { 200-299 }

ShareWeights Input_Layer {37-73} Hidden_Layer {0-99 }  Input_Layer {74-110} Hidden_Layer { 100-199 }
ShareWeights Input_Layer {37-73} Hidden_Layer { 0-99 }  Input_Layer {111-147} Hidden_Layer { 200-299 }

ShareWeights Input_Layer {74-110} Hidden_Layer { 0-99 }  Input_Layer {111-147} Hidden_Layer { 100-199 }
ShareWeights Input_Layer {74-110} Hidden_Layer { 0-99 }  Input_Layer {148-184} Hidden_Layer { 200-299 }
